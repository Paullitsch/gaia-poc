"""
GPU-Accelerated CMA-ES: Batched inference on GPU + vectorized environments.

Key optimizations:
1. PyTorch policy network with batched forward pass on GPU
2. Gymnasium vectorized environments (AsyncVectorEnv)
3. All candidates evaluated simultaneously â€” no multiprocessing Pool overhead
4. GPU handles all matrix multiplications in parallel

This should be SIGNIFICANTLY faster than CPU multiprocessing for larger networks.
"""

import numpy as np
import gymnasium as gym
import time
import os
import torch
import torch.nn as nn


class TorchPolicy(nn.Module):
    """PyTorch policy for batched GPU inference."""
    
    def __init__(self, obs_dim=8, act_dim=4, hidden1=64, hidden2=32):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(obs_dim, hidden1),
            nn.Tanh(),
            nn.Linear(hidden1, hidden2),
            nn.Tanh(),
            nn.Linear(hidden2, act_dim),
        )
        self.obs_dim = obs_dim
        self.act_dim = act_dim
        self.n_params = sum(p.numel() for p in self.parameters())
    
    def forward(self, x):
        return self.net(x)
    
    def load_flat_params(self, flat_params):
        """Load parameters from a flat numpy array."""
        idx = 0
        for p in self.parameters():
            size = p.numel()
            p.data = torch.from_numpy(
                flat_params[idx:idx+size].reshape(p.shape)
            ).float().to(p.device)
            idx += size
    
    def get_flat_params(self):
        """Get parameters as flat numpy array."""
        return np.concatenate([p.data.cpu().numpy().flatten() for p in self.parameters()])


class BatchEvaluator:
    """Evaluate multiple candidates in parallel using vectorized envs + GPU."""
    
    def __init__(self, env_name, n_envs, device, obs_dim=8, act_dim=4, 
                 hidden1=64, hidden2=32):
        self.env_name = env_name
        self.n_envs = n_envs
        self.device = device
        self.obs_dim = obs_dim
        self.act_dim = act_dim
        
        # Create vectorized environment
        self.envs = gym.vector.SyncVectorEnv([lambda e=env_name: gym.make(e) for _ in range(n_envs)])
        
        # Policy on GPU (shared, we swap params per candidate)
        self.policy = TorchPolicy(obs_dim, act_dim, hidden1, hidden2).to(device)
        self.policy.eval()
    
    def evaluate_batch(self, candidates, n_episodes=5):
        """Evaluate all candidates. Returns fitness array.
        
        Strategy: run candidates sequentially but episodes in parallel via vectorized env.
        Each candidate gets n_episodes parallel environments.
        """
        fitnesses = []
        
        for params in candidates:
            self.policy.load_flat_params(params)
            fitness = self._evaluate_one(n_episodes)
            fitnesses.append(fitness)
        
        return np.array(fitnesses)
    
    def evaluate_batch_parallel(self, candidates, n_episodes=5):
        """Evaluate candidates in batches using the vectorized env.
        
        Run n_envs environments in parallel, cycling through candidates.
        Much faster for large populations.
        """
        n_candidates = len(candidates)
        all_rewards = np.zeros(n_candidates)
        episodes_done = np.zeros(n_candidates, dtype=int)
        
        # Process in chunks of n_envs
        # Each env runs one episode for one candidate
        jobs = []  # (candidate_idx, episode_idx)
        for c in range(n_candidates):
            for e in range(n_episodes):
                jobs.append((c, e))
        
        # Process jobs in batches of n_envs
        job_idx = 0
        while job_idx < len(jobs):
            batch_size = min(self.n_envs, len(jobs) - job_idx)
            batch_jobs = jobs[job_idx:job_idx + batch_size]
            job_idx += batch_size
            
            # We need a separate env batch for this
            # Since vectorized env has fixed size, use it directly
            if batch_size < self.n_envs:
                # Pad with first candidate
                batch_jobs = batch_jobs + [(batch_jobs[0][0], 0)] * (self.n_envs - batch_size)
            
            # Reset all envs
            obs_np, _ = self.envs.reset()
            
            # Track per-env state
            candidate_indices = [j[0] for j in batch_jobs]
            env_rewards = np.zeros(self.n_envs)
            env_done = np.zeros(self.n_envs, dtype=bool)
            
            # Load all candidate params into a batch tensor for fast switching
            param_tensors = {}
            for ci in set(candidate_indices):
                param_tensors[ci] = candidates[ci]
            
            for step in range(1000):
                if env_done.all():
                    break
                
                # Group envs by candidate for batched inference
                # For simplicity, process each unique candidate
                actions = np.zeros(self.n_envs, dtype=int)
                
                for ci in set(candidate_indices):
                    mask = np.array([candidate_indices[i] == ci and not env_done[i] 
                                    for i in range(self.n_envs)])
                    if not mask.any():
                        continue
                    
                    self.policy.load_flat_params(param_tensors[ci])
                    with torch.no_grad():
                        obs_tensor = torch.from_numpy(obs_np[mask]).float().to(self.device)
                        logits = self.policy(obs_tensor)
                        batch_actions = logits.argmax(dim=1).cpu().numpy()
                    
                    actions[mask] = batch_actions
                
                # Step all envs
                obs_np, rewards, terminated, truncated, _ = self.envs.step(actions)
                done = terminated | truncated
                
                for i in range(self.n_envs):
                    if not env_done[i]:
                        env_rewards[i] += rewards[i]
                        if done[i]:
                            env_done[i] = True
            
            # Accumulate results
            for i in range(min(batch_size, len(batch_jobs))):
                ci = batch_jobs[i][0]
                all_rewards[ci] += env_rewards[i]
                episodes_done[ci] += 1
        
        # Average
        mask = episodes_done > 0
        all_rewards[mask] /= episodes_done[mask]
        return all_rewards
    
    def _evaluate_one(self, n_episodes):
        """Evaluate current policy params for n_episodes using vectorized env."""
        # Use up to n_episodes envs in parallel
        n_envs = min(n_episodes, self.n_envs)
        
        total_reward = 0.0
        episodes_completed = 0
        
        # Reset
        obs_np, _ = self.envs.reset()
        env_rewards = np.zeros(self.n_envs)
        env_done = np.zeros(self.n_envs, dtype=bool)
        
        for step in range(1000):
            active = ~env_done[:n_envs]
            if not active.any():
                break
            
            with torch.no_grad():
                obs_t = torch.from_numpy(obs_np[:n_envs][active]).float().to(self.device)
                logits = self.policy(obs_t)
                actions_active = logits.argmax(dim=1).cpu().numpy()
            
            # Build full action array
            full_actions = np.zeros(self.n_envs, dtype=int)
            active_indices = np.where(active)[0]
            for ai, fi in enumerate(active_indices):
                full_actions[fi] = actions_active[ai]
            
            obs_np, rewards, terminated, truncated, _ = self.envs.step(full_actions)
            done = terminated | truncated
            
            for i in range(n_envs):
                if not env_done[i]:
                    env_rewards[i] += rewards[i]
                    if done[i]:
                        env_done[i] = True
                        episodes_completed += 1
                        total_reward += env_rewards[i]
        
        # Handle any still running
        for i in range(n_envs):
            if not env_done[i]:
                episodes_completed += 1
                total_reward += env_rewards[i]
        
        return total_reward / max(episodes_completed, 1)
    
    def close(self):
        self.envs.close()


class CMAES:
    """CMA-ES (same as before but used with GPU evaluator)."""
    def __init__(self, n_params, sigma0=0.5, pop_size=None):
        self.n = n_params
        self.sigma = sigma0
        self.mean = np.random.randn(n_params) * 0.1
        self.lam = pop_size or (4 + int(3 * np.log(n_params)))
        self.mu = self.lam // 2
        weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))
        self.weights = weights / weights.sum()
        self.mueff = 1.0 / np.sum(self.weights ** 2)
        self.cs = (self.mueff + 2) / (n_params + self.mueff + 5)
        self.ds = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (n_params + 1)) - 1) + self.cs
        self.ps = np.zeros(n_params)
        self.cc = (4 + self.mueff / n_params) / (n_params + 4 + 2 * self.mueff / n_params)
        self.c1 = 2 / ((n_params + 1.3) ** 2 + self.mueff)
        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1 / self.mueff) / ((n_params + 2) ** 2 + self.mueff))
        self.pc = np.zeros(n_params)
        self.use_diagonal = n_params > 2000
        if self.use_diagonal:
            self.C_diag = np.ones(n_params)
        else:
            self.C = np.eye(n_params)
        self.chi_n = np.sqrt(n_params) * (1 - 1 / (4 * n_params) + 1 / (21 * n_params ** 2))
        self.gen = 0

    def ask(self):
        if self.use_diagonal:
            std = self.sigma * np.sqrt(self.C_diag)
            return [self.mean + std * np.random.randn(self.n) for _ in range(self.lam)]
        try:
            A = np.linalg.cholesky(self.C)
        except np.linalg.LinAlgError:
            self.C = np.eye(self.n); A = np.eye(self.n)
        return [self.mean + self.sigma * (A @ np.random.randn(self.n)) for _ in range(self.lam)]

    def tell(self, solutions, fitnesses):
        idx = np.argsort(fitnesses)[::-1]
        selected = np.array([solutions[i] for i in idx[:self.mu]])
        old_mean = self.mean.copy()
        self.mean = self.weights @ selected
        if self.use_diagonal:
            invsqrt = 1.0 / np.sqrt(self.C_diag)
            self.ps = (1-self.cs)*self.ps + np.sqrt(self.cs*(2-self.cs)*self.mueff)*invsqrt*(self.mean-old_mean)/self.sigma
        else:
            try: Cinv = np.linalg.inv(np.linalg.cholesky(self.C))
            except: Cinv = np.eye(self.n)
            self.ps = (1-self.cs)*self.ps + np.sqrt(self.cs*(2-self.cs)*self.mueff)*Cinv@(self.mean-old_mean)/self.sigma
        hs = int(np.linalg.norm(self.ps)/np.sqrt(1-(1-self.cs)**(2*(self.gen+1))) < (1.4+2/(self.n+1))*self.chi_n)
        self.pc = (1-self.cc)*self.pc + hs*np.sqrt(self.cc*(2-self.cc)*self.mueff)*(self.mean-old_mean)/self.sigma
        artmp = (selected - old_mean) / self.sigma
        if self.use_diagonal:
            self.C_diag = (1-self.c1-self.cmu)*self.C_diag + self.c1*(self.pc**2+(1-hs)*self.cc*(2-self.cc)*self.C_diag) + self.cmu*np.sum(self.weights[:,None]*artmp**2, axis=0)
            self.C_diag = np.maximum(self.C_diag, 1e-20)
        else:
            self.C = (1-self.c1-self.cmu)*self.C + self.c1*(np.outer(self.pc,self.pc)+(1-hs)*self.cc*(2-self.cc)*self.C) + self.cmu*sum(w*np.outer(a,a) for w,a in zip(self.weights,artmp))
        self.sigma *= np.exp((self.cs/self.ds)*(np.linalg.norm(self.ps)/self.chi_n-1))
        self.sigma = np.clip(self.sigma, 1e-20, 10.0)
        self.gen += 1


def run(params=None, device="cpu", callback=None):
    """Run GPU-accelerated CMA-ES.
    
    Args:
        params: dict with max_evals, sigma0, eval_episodes, hidden1, hidden2, env_name
        device: "cuda" for GPU, "cpu" for fallback
        callback: progress callback
    """
    params = params or {}
    max_evals = params.get("max_evals", 100000)
    sigma0 = params.get("sigma0", 0.5)
    eval_episodes = params.get("eval_episodes", 5)
    hidden1 = params.get("hidden1", 64)
    hidden2 = params.get("hidden2", 32)
    env_name = params.get("env_name", "LunarLander-v3")
    n_envs = params.get("n_envs", 32)  # Parallel environments
    
    # Auto-detect device
    if device == "cpu" and torch.cuda.is_available():
        device = "cuda"
    dev = torch.device(device)
    
    # Determine obs/act dims
    test_env = gym.make(env_name)
    obs_dim = test_env.observation_space.shape[0]
    if hasattr(test_env.action_space, 'n'):
        act_dim = test_env.action_space.n
        discrete = True
    else:
        act_dim = test_env.action_space.shape[0]
        discrete = False
    test_env.close()
    
    print(f"ðŸ–¥ï¸  GPU CMA-ES on {dev}")
    print(f"Environment: {env_name} (obs={obs_dim}, act={act_dim})")
    print(f"Parallel envs: {n_envs}")
    
    evaluator = BatchEvaluator(env_name, n_envs, dev, obs_dim, act_dim, hidden1, hidden2)
    n_params = evaluator.policy.n_params
    print(f"Network: {n_params} params | Budget: {max_evals} evals")
    
    cma = CMAES(n_params, sigma0=sigma0)
    print(f"CMA-ES population: {cma.lam}")
    
    best_ever = -float("inf")
    best_params = None
    total_evals = 0
    start_time = time.time()
    
    while total_evals < max_evals:
        candidates = cma.ask()
        
        # GPU-accelerated evaluation
        fitnesses = evaluator.evaluate_batch(candidates, n_episodes=eval_episodes)
        total_evals += len(candidates) * eval_episodes
        
        cma.tell(candidates, fitnesses)
        
        gen_best = np.max(fitnesses)
        gen_mean = np.mean(fitnesses)
        if gen_best > best_ever:
            best_ever = gen_best
            best_params = candidates[np.argmax(fitnesses)].copy()
        
        elapsed = time.time() - start_time
        evals_per_sec = total_evals / elapsed if elapsed > 0 else 0
        
        entry = {
            "generation": cma.gen,
            "best_fitness": float(gen_best),
            "best_ever": float(best_ever),
            "mean_fitness": float(gen_mean),
            "std_fitness": float(np.std(fitnesses)),
            "sigma": float(cma.sigma),
            "total_evals": total_evals,
            "elapsed": round(elapsed, 1),
        }
        if callback:
            callback(entry)
        
        solved = "âœ… SOLVED!" if best_ever >= 200 else ""
        print(f"Gen {cma.gen:4d} | Best: {gen_best:8.1f} | Ever: {best_ever:8.1f} | "
              f"Mean: {gen_mean:8.1f} | Ïƒ: {cma.sigma:.4f} | Evals: {total_evals:6d} | "
              f"{evals_per_sec:.0f} e/s | {elapsed:6.1f}s {solved}")
        
        if best_ever >= 200:
            print(f"\nðŸŽ‰ðŸ–¥ï¸  SOLVED with GPU CMA-ES! Score: {best_ever:.1f}")
            print(f"   Speed: {evals_per_sec:.0f} evals/sec")
            break
    
    evaluator.close()
    
    if best_params is not None:
        # Final evaluation
        eval2 = BatchEvaluator(env_name, 20, dev, obs_dim, act_dim, hidden1, hidden2)
        eval2.policy.load_flat_params(best_params)
        final_scores = [eval2._evaluate_one(1) for _ in range(20)]
        eval2.close()
        final_mean, final_std = float(np.mean(final_scores)), float(np.std(final_scores))
    else:
        final_mean = final_std = 0.0
    
    elapsed = time.time() - start_time
    return {
        "method": "GPU CMA-ES",
        "device": str(dev),
        "best_ever": float(best_ever),
        "final_mean": final_mean, "final_std": final_std,
        "total_evals": total_evals,
        "generations": cma.gen,
        "n_params": n_params,
        "evals_per_second": round(total_evals / elapsed, 1),
        "elapsed_seconds": round(elapsed, 1),
        "solved": best_ever >= 200,
        "n_envs": n_envs,
    }


if __name__ == "__main__":
    import json
    result = run(params={"max_evals": 50000, "n_envs": 32})
    print(f"\nResult: {json.dumps(result, indent=2)}")

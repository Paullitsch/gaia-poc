# GAIA â€” Global Artificial Intelligence Architecture

> Gradientenfreie Optimierung als Alternative zur Backpropagation

**Status:** Phase 10 â€” Atari + GPU Acceleration ğŸ®

---

## ğŸ¯ Projektziel

Beweisen, dass neuronale Netze **ohne Backpropagation** trainiert werden kÃ¶nnen â€” und eine verteilte Infrastruktur bauen, die das auf beliebig vielen Maschinen parallelisiert.

## ğŸ“Š Ergebnisse

### LunarLander-v3 â€” 9/10 Methoden gelÃ¶st âœ…

| Methode | Best Score | Evals | Backprop? |
|---------|-----------|-------|-----------|
| ğŸ† Curriculum CMA-ES | **+341.9** | 8K | âŒ |
| Neuromod CMA-ES | **+264.5** | 13K | âŒ |
| Neuromod Island | **+256.3** | 48K | âŒ |
| CMA-ES | **+235.3** | 12K | âŒ |
| Island Model | **+235.0** | 46K | âŒ |
| GPU CMA-ES | **+232.5** | 17K | âŒ |
| Scaling (XL) | **+215.5** | 12K | âŒ |
| Hybrid CMA+FF | **+209.5** | 9K | âŒ |
| OpenAI-ES | **+206.6** | 56K | âŒ |
| Island Advanced | **+201.7** | 70K | âŒ |
| Indirect Encoding | +9.1 | â€” | âŒ |
| PPO (Baseline) | +264.8 | â€” | âœ… |

### BipedalWalker-v3 â€” GELÃ–ST âœ…

| Methode | Best Score | Evals |
|---------|-----------|-------|
| ğŸ† CMA-ES | **+566.6** | 40K |
| Curriculum CMA-ES | **+338.5** | â€” |
| CMA-ES (standard) | **+265.9** | 8K |

## ğŸ“š Wiki-Seiten

### Theorie & Forschung
- [[Hypothesen-Evolution]] â€” Von v1 bis v4
- [[Experimentelle Phasen]] â€” Phase 1-10 im Detail
- [[Epistemische Architektur]] â€” Was wir wissen vs. vermuten
- [[Methoden]] â€” Alle 11 Methoden erklÃ¤rt

### Infrastruktur
- [[Architektur]] â€” Server-Worker-System
- [[Server API]] â€” REST Endpoints
- [[Deployment]] â€” Docker, Binaries, Setup
- [[Auto-Update System]] â€” Self-Updating Worker

### Phasen
- [[Phase 8 Plan]] â€” BipedalWalker + Auto-Update
- [[Phase 9 Dezentralisierung]] â€” Island Model + P2P Gossip
- [[Phase 10 Atari]] â€” CNN + GPU Acceleration
- [[Scaling Hypothesen]] â€” Wo liegen die Grenzen?

---

**Repository:** https://github.com/Paullitsch/gaia-poc
**Dashboard:** https://gaia.kndl.at/
**Lizenz:** MIT

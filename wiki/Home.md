# GAIA â€” Global Artificial Intelligence Architecture

> Gradientenfreie Optimierung als Alternative zur Backpropagation

**Status:** Phase 8 gestartet â€” BipedalWalker + Auto-Update Infrastruktur ğŸš€

---

## ğŸ¯ Projektziel

Beweisen, dass neuronale Netze **ohne Backpropagation** trainiert werden kÃ¶nnen â€” und eine verteilte Infrastruktur bauen, die das auf beliebig vielen Maschinen parallelisiert.

## ğŸ“Š Ergebnisse

| Methode | Best Score | Status |
|---------|-----------|--------|
| Curriculum + CMA-ES | **+274.0** | âœ… SOLVED |
| CMA-ES | **+235.3** | âœ… SOLVED |
| OpenAI-ES | **+206.6** | âœ… SOLVED |
| Indirect Encoding | -9.4 | âŒ |

## ğŸ“š Wiki-Seiten

### Theorie & Forschung
- [[Hypothesen-Evolution]] â€” Von v1 bis v4
- [[Experimentelle Phasen]] â€” Alle 7 Phasen im Detail
- [[Epistemische Architektur]] â€” Was wir wissen vs. vermuten
- [[Methoden]] â€” CMA-ES, OpenAI-ES, Forward-Forward, Neuromodulation

### Infrastruktur
- [[Architektur]] â€” Server-Worker-System
- [[Server API]] â€” REST Endpoints
- [[Dashboard]] â€” Web UI Features
- [[Deployment]] â€” Docker, Binaries, Setup

### Roadmap
- [[Phase 8 Plan]] â€” BipedalWalker + Auto-Update Infrastruktur
- [[Auto-Update System]] â€” Self-Updating Worker (v0.4.x)
- [[Scaling Hypothesen]] â€” Wo liegen die Grenzen?

---

**Repository:** https://github.com/Paullitsch/gaia-poc
**Dashboard:** https://gaia.kndl.at/
**Lizenz:** MIT

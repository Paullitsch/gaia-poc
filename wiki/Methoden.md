# Methoden

> Alle Methoden sind environment-agnostisch ‚Äî laufen auf jedem Gymnasium-Environment.
> Ab v0.7.0 auch nativ in Rust verf√ºgbar (7 Methoden + 3 Environments).

## Gradientenfreie Methoden (GAIA)

### CMA-ES (`cma_es`)
**Covariance Matrix Adaptation Evolution Strategy** ‚Äî Gold-Standard der gradientenfreien Optimierung.

- Population von Parametervektoren ‚Üí evaluate ‚Üí Selektion ‚Üí Kovarianzmatrix anpassen
- O(n¬≤) Speicher f√ºr volle Kovarianzmatrix, Diagonal-Modus ab >2000 params
- **St√§rke:** Sehr sample-effizient bei kleinen-mittleren Netzen
- **Schw√§che:** Skaliert nicht √ºber ~50K params (Speicher)
- **Rust:** ‚úÖ Portiert

### OpenAI-ES (`openai_es`)
**OpenAI Evolution Strategies** mit antithetischem Sampling.

- Perturbiere Parameter mit Gau√ü-Rauschen, nutze Reward als "Gradient-Sch√§tzer"
- Antithetisch: teste +Œµ und -Œµ ‚Üí varianzreduziert
- O(n) Speicher ‚Äî skaliert beliebig
- **St√§rke:** Massiv parallelisierbar, skaliert bei gro√üen Netzen
- **Schw√§che:** Weniger sample-effizient als CMA-ES
- **Rust:** ‚úÖ Portiert

### Curriculum CMA-ES (`curriculum`)
CMA-ES mit **Reward Shaping + Curriculum Learning**.

- Difficulty ramp: 0.3 ‚Üí 1.0 √ºber Training
- Geformte Rewards: Survival-Bonus, Geschwindigkeits-Bonus, Aufrecht-Bonus
- Environment-spezifisches Shaping (LunarLander, BipedalWalker)
- **St√§rke:** Sample-effizienteste Methode (8K Evals!)
- **Biologie:** Curriculum Learning spiegelt kindliche Entwicklung wider
- **Rust:** ‚úÖ Portiert

### Neuromod CMA-ES (`neuromod`)
CMA-ES + **Neuromodulatory Plasticity**.

- Netzwerk hat zus√§tzliche Plastizit√§ts-Parameter
- Synapsen ver√§ndern sich w√§hrend der Evaluation (Hebbian-artig)
- CMA-ES optimiert initiale Gewichte + Plastizit√§tsregeln
- **St√§rke:** Biologisch plausibelste Methode
- **Key Finding:** +80 bei 2K Evals ‚Üí +264.5 bei 200K Evals
- **Rust:** ‚úÖ Portiert

### Island Model (`island_model`)
**4 CMA-ES Populationen** mit periodischer Migration.

- 4 Inseln mit verschiedenen Sigmas (0.3, 0.5, 0.8, 1.2)
- Migration alle 10 Gen: bestes Individuum ‚Üí n√§chste Insel
- **St√§rke:** Diversit√§t durch parallele Suche, robust gegen lokale Optima
- **Finding:** Neuromod Island > Neuromod > Islands
- **Rust:** ‚úÖ Portiert

### Island Advanced (`island_advanced`)
**6 heterogene Inseln** mit adaptiver Migration.

- Fully connected Topologie
- Adaptive Migrationsrate (mehr Migration bei niedriger Diversit√§t)
- Migration-Tournament: nur akzeptieren wenn besser als 80% des Besten

### Neuromod Island (`neuromod_island`)
**Neuromodulation + Island Model** kombiniert.

- Plastische Netze auf mehreren Inseln
- Beweist: lokale Lernregeln + Populationsdynamik = stark
- **Best Result:** +256.3 auf LunarLander

### Hybrid CMA+FF (`hybrid_cma_ff`)
**CMA-ES + Forward-Forward** Local Learning.

- CMA-ES optimiert initiale Gewichte + FF-Hyperparameter
- "Goodness"-basiert: positive Erfahrungen ‚Üí verst√§rken, negative ‚Üí abschw√§chen

### Indirect Encoding (`indirect_encoding`)
**CPPN-basiert** ‚Äî ein kleines Netz *generiert* die Policy-Gewichte.

- ~625 Genom-Parameter ‚Üí ~3000 Policy-Parameter
- Inspiriert von Biologie: DNA encodiert Entwicklungsprogramm
- **Status:** Schw√§chste Methode (+9.1 auf LunarLander)

### Scaling Test (`scaling_test`)
**Network-Gr√∂√üen-Experiment** ‚Äî testet CMA-ES bei verschiedenen Netzgr√∂√üen.

- Configs: 1K, 10K, 33K, 100K Parameter
- **LunarLander:** Alle l√∂sen es ‚Üí zu einfach f√ºr Breakpoint
- **BipedalWalker:** Tests laufen (erwarteter Breakpoint)
- **Rust:** ‚úÖ Portiert

### Meta-Learning (`meta_learning`) üÜï
**CMA-ES evolves Lernregeln** statt nur Gewichte.

- Genom enth√§lt Gewichte + Lernregel-Parameter (eta, decay, modulation gains)
- Netzwerk lernt *w√§hrend* der Evaluation basierend auf evolvierten Regeln
- Verschmilzt Evolution mit lebenslangem Lernen
- **LunarLander:** +245.2 ‚Äî zweitbeste Methode nach Curriculum!
- **Rust:** ‚úÖ Portiert

### Pure Meta-Learning (`meta_learning_pure`) üÜï
**Evolve NUR Lernregeln** ‚Äî der biologischste Ansatz.

- Nur 21 Parameter im Genom (Lernregel-Koeffizienten)
- Gewichte werden zuf√§llig initialisiert ‚Üí m√ºssen durch Lernregeln konvergieren
- Spiegelt Biologie wider: Gene kodieren WIE gelernt wird, nicht WAS
- **Status:** Jobs laufen auf LunarLander + BipedalWalker
- **Bedeutung:** Wenn das funktioniert ‚Üí Skalierung zu beliebig gro√üen Netzen m√∂glich (Genom bleibt klein)

## Backprop-Baseline (Kontrollgruppe)

### PPO Baseline (`ppo_baseline`) ‚ö†Ô∏è
**Proximal Policy Optimization** ‚Äî gradient-basiert.

- Standard-RL mit Backpropagation
- Gleiche Netzwerk-Architektur (fairer Vergleich)
- **‚ö†Ô∏è Nutzt Backpropagation** ‚Äî klar gekennzeichnet
- **LunarLander:** +59.7 (schlechter als 7 gradientenfreie Methoden!)
- **BipedalWalker:** +145.9

## Methoden-Vergleich

### LunarLander-v3 (100K Evals)

```
Curriculum     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  790.1  üß¨
Meta-Learning  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              245.2  üß¨
Scaling 10K    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                               227.2  üß¨
Neuromod       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                217.6  üß¨
CMA-ES         ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                214.4  üß¨
Neuromod Island‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                200.3  üß¨
Island Model   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                  175.9  üß¨
OpenAI-ES      ‚ñà‚ñà‚ñà                                        73.4  üß¨
PPO            ‚ñà‚ñà                                         59.7  ‚ö°
```

üß¨ = Gradientenfrei | ‚ö° = Backpropagation

# Experimentelle Phasen

## Ãœbersicht

| Phase | Methode | Aufgabe | Best Score | Ergebnis |
|-------|---------|---------|-----------|----------|
| 1 | Reine Evolution | CartPole | 500/500 | âœ… GelÃ¶st |
| 2 | Evolution + Hebbisch | LunarLander | +59.7 | âŒ Skaliert nicht |
| 3 | Forward-Forward | LunarLander | 50-70% v. Backprop | ğŸŸ¡ Teilweise |
| 4 | Meta-PlastizitÃ¤t | LunarLander | -50.4 | ğŸŸ¡ SchlÃ¤gt naive BP |
| 5 | Neuromodulation | LunarLander | +80.0 | ğŸŸ¡ Durchbruch |
| 6 | Deep Neuromod + PPO | LunarLander | +57.8 / +264.8 | ğŸŸ¡ PPO gewinnt |
| 7 | CMA-ES + Compute | LunarLander | **+274.0** | âœ… **GELÃ–ST** |

## Phase 1: CartPole (722 Parameter)

**Frage:** Kann Evolution neuronale Netze trainieren?
**Antwort:** Ja, aber 20x weniger effizient als Backprop.

Alle evolutionÃ¤ren Varianten (rein, Hebbisch, Reward-Hebbisch) lÃ¶sten CartPole (500/500). REINFORCE brauchte nur 217 Episoden vs. 4.500 bei Evolution.

## Phase 2: LunarLander (6.948 Parameter)

**Frage:** Skaliert Evolution auf schwerere Probleme?
**Antwort:** Nein. Skalierungswand bei ~7K Parametern.

Bester Score: +59.7 (Reward-Hebbisch). Weit unter dem LÃ¶sungsschwellenwert von +200. Die Fitness-Landschaft wird zu komplex fÃ¼r gradientenfreie Suche im Gewichtsraum.

## Phase 3: Forward-Forward (10.000 Parameter)

**Frage:** KÃ¶nnen lokale Lernregeln Backprop ersetzen?
**Antwort:** Sie kommen auf 50-70%.

Hintons Forward-Forward-Algorithmus, erweitert durch evolutionÃ¤re Hyperparameter-Optimierung. Ãœberraschend nahe an Backprop, aber die LÃ¼cke bleibt signifikant.

## Phase 4: Meta-PlastizitÃ¤t (11.600 Parameter)

**Frage:** Was wenn Evolution Lernregeln statt Gewichte optimiert?
**Antwort:** SchlÃ¤gt naive Backprop!

Meta-PlastizitÃ¤t (-50.4) Ã¼bertraf REINFORCE (-158.4). Evolution als Meta-Lernalgorithmus ist der richtige Ansatz.

## Phase 5: Neuromodulation (20.000 Parameter)

**Frage:** Helfen biologisch inspirierte Modulationssignale?
**Antwort:** Dramatischer Durchbruch (+80.0).

Drei Signale (Dopamin, TD-Error, NovitÃ¤t) modulieren schichtenspezifisch die PlastizitÃ¤t. 3x compute-effizienter als Meta-PlastizitÃ¤t. Erster positiver Score in GAIA-Geschichte.

## Phase 6: Deep Neuromodulation (23K+ Parameter)

**Frage:** KÃ¶nnen wir die Neuromodulation vertiefen?
**Antwort:** Ja, aber PPO bleibt Ã¼berlegen.

5 Neuromodulationssignale + Eligibility Traces: +57.8. PPO Baseline: +264.8. Die Credit-Assignment-LÃ¼cke zwischen lokalem FF-Lernen und globalem Backprop bleibt das fundamentale Hindernis.

## Phase 7: CMA-ES + Compute (2.788 Parameter) â­

**Frage:** Was passiert mit genug Compute?
**Antwort:** GELÃ–ST. +274.0 ohne Backpropagation.

Kleineres Netzwerk (2.788 statt 20K Parameter), aber massiv mehr Compute (100K Evaluierungen statt 10K). CMA-ES lernt die Kovarianzstruktur und findet optimale Gewichte.

**SchlÃ¼sseleinblick:** Das Netzwerk war zu groÃŸ, nicht der Algorithmus zu schwach. CMA-ES skaliert O(nÂ²) mit der Parameterzahl â€” ein kleineres Netz mit mehr Compute war der Weg.

### Lernkurve CMA-ES (Phase 7)

```
Score
+274 â”‚                                          â—
+200 â”‚â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ SOLVED â”€ â”€ â”€ â”€/â”€ â”€
+150 â”‚                                    â—  /
+100 â”‚                               â—  /
 +50 â”‚                          â—  /
   0 â”‚                     â—  /
 -50 â”‚                â—  /
-100 â”‚           â—  /
-150 â”‚      â—  /
-200 â”‚  â— /
     â””â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€
        10 20 30 40 50 60 70 80 90
                  Generation
```

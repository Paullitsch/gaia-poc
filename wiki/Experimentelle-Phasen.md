# Experimentelle Phasen

## Ãœbersicht

| Phase | Methode | Aufgabe | Best Score | Ergebnis |
|-------|---------|---------|-----------|----------|
| 1 | Reine Evolution | CartPole | 500/500 | âœ… GelÃ¶st |
| 2 | Evolution + Hebbisch | LunarLander | +59.7 | âŒ Skaliert nicht |
| 3 | Forward-Forward | LunarLander | 50-70% v. Backprop | ğŸŸ¡ Teilweise |
| 4 | Meta-PlastizitÃ¤t | LunarLander | -50.4 | ğŸŸ¡ SchlÃ¤gt naive BP |
| 5 | Neuromodulation | LunarLander | +80.0 | ğŸŸ¡ Durchbruch |
| 6 | Deep Neuromod + PPO | LunarLander | +57.8 / +264.8 | ğŸŸ¡ PPO gewinnt |
| 7 | CMA-ES + Compute | LunarLander | **+274.0** | âœ… **GELÃ–ST** |
| 8 | BipedalWalker + Infra | BipedalWalker | **+566.6** | âœ… **GELÃ–ST** |
| 9 | Dezentralisierung | Island Model + P2P | **+256.3** | âœ… Abgeschlossen |
| 10 | Scaling + Meta-Learning | Benchmarks + Rust | ğŸ”¬ | In Arbeit |

## Phase 1: CartPole (722 Parameter)

**Frage:** Kann Evolution neuronale Netze trainieren?
**Antwort:** Ja, aber 20x weniger effizient als Backprop.

Alle evolutionÃ¤ren Varianten (rein, Hebbisch, Reward-Hebbisch) lÃ¶sten CartPole (500/500). REINFORCE brauchte nur 217 Episoden vs. 4.500 bei Evolution.

## Phase 2: LunarLander (6.948 Parameter)

**Frage:** Skaliert Evolution auf schwerere Probleme?
**Antwort:** Nein. Skalierungswand bei ~7K Parametern.

Bester Score: +59.7 (Reward-Hebbisch). Weit unter dem LÃ¶sungsschwellenwert von +200. Die Fitness-Landschaft wird zu komplex fÃ¼r gradientenfreie Suche im Gewichtsraum.

## Phase 3: Forward-Forward (10.000 Parameter)

**Frage:** KÃ¶nnen lokale Lernregeln Backprop ersetzen?
**Antwort:** Sie kommen auf 50-70%.

Hintons Forward-Forward-Algorithmus, erweitert durch evolutionÃ¤re Hyperparameter-Optimierung.

## Phase 4: Meta-PlastizitÃ¤t (11.600 Parameter)

**Frage:** Was wenn Evolution Lernregeln statt Gewichte optimiert?
**Antwort:** SchlÃ¤gt naive Backprop!

Meta-PlastizitÃ¤t (-50.4) Ã¼bertraf REINFORCE (-158.4). Evolution als Meta-Lernalgorithmus ist der richtige Ansatz.

## Phase 5: Neuromodulation (20.000 Parameter)

**Frage:** Helfen biologisch inspirierte Modulationssignale?
**Antwort:** Dramatischer Durchbruch (+80.0).

Drei Signale (Dopamin, TD-Error, NovitÃ¤t) modulieren schichtenspezifisch die PlastizitÃ¤t. 3x compute-effizienter als Meta-PlastizitÃ¤t.

## Phase 6: Deep Neuromodulation (23K+ Parameter)

**Frage:** KÃ¶nnen wir die Neuromodulation vertiefen?
**Antwort:** PPO bleibt Ã¼berlegen. Die Credit-Assignment-LÃ¼cke bleibt.

5 Neuromodulationssignale + Eligibility Traces: +57.8. PPO Baseline: +264.8.

## Phase 7: CMA-ES + Compute (2.788 Parameter) â­

**Frage:** Was passiert mit genug Compute?
**Antwort:** GELÃ–ST. +274.0 ohne Backpropagation.

Kleineres Netzwerk (2.788 statt 20K Parameter), aber massiv mehr Compute (100K Evaluierungen). CMA-ES lernt die Kovarianzstruktur und findet optimale Gewichte.

**SchlÃ¼sseleinblick:** Das Netzwerk war zu groÃŸ, nicht der Algorithmus zu schwach.

## Phase 8: BipedalWalker + Infrastruktur âœ…

**Frage:** Skalieren gradientenfreie Methoden auf kontinuierliche Kontrolle?
**Antwort:** Ja! CMA-ES lÃ¶st BipedalWalker mit +566.6.

BipedalWalker-v3: 24D Observation, 4 kontinuierliche Aktoren, Solved Threshold 300.

### Ergebnisse

| Methode | Best Score | Evals |
|---------|-----------|-------|
| CMA-ES (patience=500) | **+566.6** | 40K |
| Curriculum CMA-ES | **+338.5** | â€” |
| CMA-ES (standard) | **+265.9** | 8K |

### Infrastruktur-Meilensteine
- **Auto-Update System** (v0.4.0â†’v0.5.9): Worker aktualisiert sich selbst
- **Experiment-Sync**: Neue Experiments automatisch verteilt
- **Environment-agnostische Methoden**: Alle 11 Methoden laufen auf jedem Env
- **Dashboard**: Benchmarks-Tab, Leaderboard, Learning Curves

## Phase 9: Dezentralisierung âœ…

**Frage:** Kann dezentrale Evolution mithalten?
**Antwort:** Ja â€” Neuromod Island (+256.3) Ã¼bertrifft Einzelpopulationen.

### Kernresultate
- **Island Model** mit 4 CMA-ES Populationen + Migration
- **P2P Gossip Protocol** implementiert (Port 7435)
- **Neuromod Island** (+256.3) > Neuromod standalone (+264.5 solo, +217.6 Benchmark) > Islands (+175.9)
- Kombination von lokalen Lernregeln + Populationsdynamik ist stÃ¤rker als beides allein

### Benchmark Architecture Refactor
- **Environment + Method getrennt**: Jobs haben `environment` Feld
- **Shared PolicyNetwork + evaluate()**: Alle Methoden importieren aus `cma_es.py`
- **PPO Baseline** als Backprop-Kontrollgruppe
- Dashboard zeigt ğŸ§¬ GRAD-FREE vs âš¡ BACKPROP Badges

## Phase 10: Scaling + Meta-Learning + Rust ğŸ”¬

Phase 10 hat drei StrÃ¤nge:

### Strang 1: Atari (deprioritized)
CNN Policy + GPU Batch Evaluation fÃ¼r Pixel-Envs implementiert. **Erkenntnis:** `env.step()` ist der Bottleneck, nicht GPU. Atari war eine Ablenkung â€” der Weg fÃ¼hrt Ã¼ber Meta-Learning, nicht grÃ¶ÃŸere Netze.

### Strang 2: Skalierungstests + Meta-Learning
- **Scaling Tests**: CMA-ES bei 1Kâ†’100K Parametern (LunarLander alle gelÃ¶st â†’ zu einfach)
- **BipedalWalker Scaling**: Tests laufen (1K, 10K, 33K, 100K params)
- **Meta-Learning**: ES evolves Lernregeln statt Gewichte â†’ biologischer Ansatz
- **Pure Meta-Learning**: Nur 21 Lernregel-Parameter, zufÃ¤llige Gewichts-Init

### Strang 3: Rust-Migration ğŸ¦€
- **Worker v0.7.0**: Alle Python-Experimente durch native Rust ersetzt
- 7 Methoden + 3 Environments in Rust portiert
- Speedups: CartPole 13.6Ã—, LunarLander 10.4Ã— (parallel)
- Rayon fÃ¼r parallele Population-Evaluation
- Details: [[Rust-Migration]]

### KEY INSIGHT
> Der Weg zur SingularitÃ¤t ist nicht grÃ¶ÃŸere Netze mit ES, sondern ES evolves Lernregeln die Netze trainieren. â€” Das ist der biologische Weg.
